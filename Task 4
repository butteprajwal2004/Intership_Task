import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import numpy as np
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# GPT-2 Text Generation
def generate_text_gpt2(prompt, max_length=100):
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    output = model.generate(inputs, max_length=max_length, num_return_sequences=1)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Simple LSTM-based Text Generator
class TextDataset(Dataset):
    def __init__(self, text, seq_length):
        self.text = text
        self.seq_length = seq_length
        self.chars = sorted(set(text))
        self.char2idx = {c: i for i, c in enumerate(self.chars)}
        self.idx2char = {i: c for i, c in enumerate(self.chars)}
        self.data = [self.char2idx[c] for c in text]
    
    def __len__(self):
        return len(self.data) - self.seq_length
    
    def __getitem__(self, index):
        return (torch.tensor(self.data[index:index+self.seq_length]), 
                torch.tensor(self.data[index+1:index+self.seq_length+1]))

class LSTMTextGenerator(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):
        super(LSTMTextGenerator, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x, hidden):
        x = self.embedding(x)
        out, hidden = self.lstm(x, hidden)
        out = self.fc(out)
        return out, hidden

def train_lstm(text, epochs=10, seq_length=50):
    dataset = TextDataset(text, seq_length)
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
    vocab_size = len(dataset.chars)
    model = LSTMTextGenerator(vocab_size, 128, 256, 2)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.003)
    
    for epoch in range(epochs):
        hidden = None
        for inputs, targets in dataloader:
            optimizer.zero_grad()
            outputs, hidden = model(inputs, hidden)
            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item()}")
    
    return model, dataset

if __name__ == "__main__":
    print("GPT-2 Generated Text:")
    print(generate_text_gpt2("Artificial Intelligence is transforming the world", 100))
    
    print("\nTraining LSTM model...")
    sample_text = "This is a simple text generation model using LSTM and PyTorch. " * 10
    train_lstm(sample_text)
